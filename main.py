from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

app = FastAPI()

print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/polyglot-ko-5.8b", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("EleutherAI/polyglot-ko-5.8b", trust_remote_code=True)
model.eval()
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

class TextRequest(BaseModel):
    text: str

@app.post("/transform/")
async def transform(request: TextRequest):
    prompt = f"ë‹¤ìŒ ë¬¸ì¥ì„ ë” ìì—°ìŠ¤ëŸ½ê²Œ ë°”ê¿”ì¤˜:\n{request.text}\n\në³€í™˜ëœ ë¬¸ì¥:"

    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_length=200,
            num_beams=5,
            no_repeat_ngram_size=2,
            early_stopping=True
        )
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"original": request.text, "transformed": result.split("ë³€í™˜ëœ ë¬¸ì¥:")[-1].strip()}
